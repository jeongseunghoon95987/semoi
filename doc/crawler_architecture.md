# 크롤러 아키텍처

이 문서는 세모이 프로젝트의 크롤링 시스템 아키텍처에 대한 주요 결정 사항을 기록합니다.

## 크롤링 엔진

- **기술**: **Node.js + Puppeteer**
- **선택 이유**:
  - **동적 웹사이트 처리**: 최신 웹사이트들은 JavaScript를 통해 동적으로 컨텐츠를 로드하는 경우가 많습니다. Puppeteer는 실제 브라우저(Chromium)를 제어하는 라이브러리이므로, JavaScript가 모두 실행된 후의 최종적인 HTML을 가져올 수 있어 동적 컨텐츠 수집에 매우 강력합니다.
  - **비동기 처리**: Node.js의 비동기 I/O 모델은 여러 웹사이트를 동시에 효율적으로 크롤링하는 데 적합합니다.
  - **풍부한 생태계**: Node.js와 JavaScript 커뮤니티는 웹 스크래핑과 관련된 다양한 라이브러리와 풍부한 예제 자료를 보유하고 있습니다.

## 실행 방식

- 크롤러는 Laravel 애플리케이션과 별개의 Node.js 프로세스로 실행됩니다.
- **스케줄링**: Laravel 스케줄러(`bootstrap/app.php`에서 정의)를 통해 `app:crawl-events` Artisan 커맨드를 주기적으로 실행하여 Node.js 크롤러 스크립트를 트리거합니다.
- **수동 실행**: 관리자 페이지의 버튼을 통해 수동으로 크롤링 작업을 시작할 수 있습니다.
- **데이터 흐름 (핵심 변경)**:
    - 크롤러는 **Laravel API(`GET /api/event-sources`)를 통해 크롤링 대상(`event_sources`) 목록과 해당 규칙을 전달받습니다.**
    - 수집된 이벤트 데이터는 **Laravel API(`POST /api/events`)를 통해 Laravel 백엔드로 전송**됩니다. Laravel 백엔드는 이 데이터를 유효성 검사 후 `events` 테이블에 저장합니다.
    - **Node.js 크롤러는 데이터베이스에 직접 접근하지 않습니다.** 모든 데이터 읽기/쓰기 작업은 Laravel API를 통해서만 이루어집니다.
